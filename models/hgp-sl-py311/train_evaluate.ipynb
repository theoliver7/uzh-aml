{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T09:26:15.064063Z",
     "start_time": "2024-04-15T09:26:12.480950Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from models_new import Model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': \"cpu\",\n",
    "    'seed': 42,\n",
    "    \"batch_size\": 64,\n",
    "    'lr': 0.0001,\n",
    "    'weight_decay': 0.001,\n",
    "    'epochs': 100,\n",
    "    'patience': 1000,\n",
    "    'nhid': 128,\n",
    "    'pooling_ratio': 0.3,\n",
    "    'dropout_ratio': 0.5,\n",
    "    'sample_neighbor': True,\n",
    "    'sparse_attention': True,\n",
    "    'structure_learning': True,\n",
    "    'lamb': 1.0,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T09:26:16.713811Z",
     "start_time": "2024-04-15T09:26:15.065349Z"
    }
   },
   "id": "ef02e2be402f49d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=10)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args[\"device\"] = 'cuda'\n",
    "    \n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "dataset = TUDataset(os.path.join('../../data', \"DD\"), name=\"DD\", use_node_attr=True)\n",
    "args[\"num_classes\"] = dataset.num_classes\n",
    "args[\"num_features\"] = dataset.num_features\n",
    "\n",
    "num_test = int(len(dataset) * 0.1)\n",
    "num_train = len(dataset) - num_test\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=args[\"seed\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T09:26:16.943199Z",
     "start_time": "2024-04-15T09:26:16.715108Z"
    }
   },
   "id": "1bbb51a91e218bd",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    loss_test = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(args[\"device\"])\n",
    "        out = model(data)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        loss_test += F.nll_loss(out, data.y).item()\n",
    "    return correct / len(loader.dataset), loss_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T09:26:16.975056Z",
     "start_time": "2024-04-15T09:26:16.962634Z"
    }
   },
   "id": "5441549860e8b324",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_fold(train_idx, val_idx):\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=args[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model = Model(args).to(args[\"device\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "    min_loss = 1e10\n",
    "    patience_cnt = 0\n",
    "    val_loss_values = []\n",
    "    best_epoch = 0\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        loss_train = 0.0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(args[\"device\"])\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "        acc_train = correct / len(train_loader.dataset)\n",
    "        acc_val, loss_val = compute_test(model, val_loader)\n",
    "        print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.6f}'.format(loss_train),\n",
    "              'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "              'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "\n",
    "        val_loss_values.append(loss_val)\n",
    "        torch.save(model.state_dict(), '{}.pth'.format(epoch))\n",
    "        if val_loss_values[-1] < min_loss:\n",
    "            min_loss = val_loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        if patience_cnt == args[\"patience\"]:\n",
    "            break\n",
    "\n",
    "        files = glob.glob('*.pth')\n",
    "        for f in files:\n",
    "            epoch_nb = int(f.split('.')[0])\n",
    "            if epoch_nb < best_epoch:\n",
    "                os.remove(f)\n",
    "\n",
    "    files = glob.glob('*.pth')\n",
    "    for f in files:\n",
    "        epoch_nb = int(f.split('.')[0])\n",
    "        if epoch_nb > best_epoch:\n",
    "            os.remove(f)\n",
    "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T09:26:17.010849Z",
     "start_time": "2024-04-15T09:26:16.979167Z"
    }
   },
   "id": "e8f3686eb604ef20",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Epoch: 0001 loss_train: 9.619408 acc_train: 0.550708 loss_val: 2.756134 acc_val: 0.544601 time: 6.519970s\n",
      "Epoch: 0002 loss_train: 9.541916 acc_train: 0.596698 loss_val: 2.748063 acc_val: 0.544601 time: 12.536841s\n",
      "Epoch: 0003 loss_train: 9.453388 acc_train: 0.596698 loss_val: 2.743914 acc_val: 0.544601 time: 18.668000s\n",
      "Epoch: 0004 loss_train: 9.370406 acc_train: 0.596698 loss_val: 2.744851 acc_val: 0.544601 time: 25.067007s\n",
      "Epoch: 0005 loss_train: 9.327451 acc_train: 0.596698 loss_val: 2.749066 acc_val: 0.544601 time: 31.188390s\n",
      "Epoch: 0006 loss_train: 9.343445 acc_train: 0.596698 loss_val: 2.753709 acc_val: 0.544601 time: 37.435712s\n",
      "Epoch: 0007 loss_train: 9.312488 acc_train: 0.596698 loss_val: 2.750348 acc_val: 0.544601 time: 43.752263s\n",
      "Epoch: 0008 loss_train: 9.245900 acc_train: 0.596698 loss_val: 2.746377 acc_val: 0.544601 time: 49.807858s\n",
      "Epoch: 0009 loss_train: 9.219279 acc_train: 0.596698 loss_val: 2.750835 acc_val: 0.544601 time: 55.983502s\n",
      "Epoch: 0010 loss_train: 9.293616 acc_train: 0.596698 loss_val: 2.745874 acc_val: 0.544601 time: 61.992697s\n",
      "Epoch: 0011 loss_train: 9.197049 acc_train: 0.596698 loss_val: 2.736437 acc_val: 0.544601 time: 68.040297s\n",
      "Epoch: 0012 loss_train: 9.156830 acc_train: 0.596698 loss_val: 2.736167 acc_val: 0.544601 time: 74.103415s\n",
      "Epoch: 0013 loss_train: 9.215842 acc_train: 0.596698 loss_val: 2.722300 acc_val: 0.544601 time: 80.038469s\n",
      "Epoch: 0014 loss_train: 9.237547 acc_train: 0.596698 loss_val: 2.709681 acc_val: 0.544601 time: 86.307188s\n",
      "Epoch: 0015 loss_train: 9.149034 acc_train: 0.596698 loss_val: 2.689173 acc_val: 0.544601 time: 93.077847s\n",
      "Epoch: 0016 loss_train: 9.096480 acc_train: 0.596698 loss_val: 2.690227 acc_val: 0.544601 time: 99.465681s\n",
      "Epoch: 0017 loss_train: 9.006972 acc_train: 0.596698 loss_val: 2.677698 acc_val: 0.549296 time: 105.898329s\n",
      "Epoch: 0018 loss_train: 9.020307 acc_train: 0.596698 loss_val: 2.667429 acc_val: 0.544601 time: 113.298619s\n",
      "Epoch: 0019 loss_train: 8.889196 acc_train: 0.600236 loss_val: 2.614475 acc_val: 0.558685 time: 119.836932s\n",
      "Epoch: 0020 loss_train: 8.834390 acc_train: 0.602594 loss_val: 2.570078 acc_val: 0.596244 time: 126.222360s\n",
      "Epoch: 0021 loss_train: 8.552280 acc_train: 0.662736 loss_val: 2.510780 acc_val: 0.652582 time: 132.826334s\n",
      "Epoch: 0022 loss_train: 8.364593 acc_train: 0.676887 loss_val: 2.454784 acc_val: 0.694836 time: 139.146170s\n",
      "Epoch: 0023 loss_train: 8.122434 acc_train: 0.716981 loss_val: 2.454628 acc_val: 0.723005 time: 145.486569s\n",
      "Epoch: 0024 loss_train: 7.894549 acc_train: 0.744104 loss_val: 2.514951 acc_val: 0.694836 time: 151.619230s\n",
      "Epoch: 0025 loss_train: 7.851808 acc_train: 0.739387 loss_val: 2.476439 acc_val: 0.713615 time: 157.900340s\n",
      "Epoch: 0026 loss_train: 7.660871 acc_train: 0.744104 loss_val: 2.508800 acc_val: 0.713615 time: 164.138515s\n",
      "Epoch: 0027 loss_train: 7.716673 acc_train: 0.729953 loss_val: 2.579944 acc_val: 0.704225 time: 170.243300s\n",
      "Epoch: 0028 loss_train: 7.464159 acc_train: 0.745283 loss_val: 2.407049 acc_val: 0.732394 time: 176.369857s\n",
      "Epoch: 0029 loss_train: 7.289272 acc_train: 0.754717 loss_val: 2.403422 acc_val: 0.723005 time: 182.456521s\n",
      "Epoch: 0030 loss_train: 7.309441 acc_train: 0.746462 loss_val: 2.420775 acc_val: 0.732394 time: 188.564747s\n",
      "Epoch: 0031 loss_train: 7.362744 acc_train: 0.734670 loss_val: 2.438968 acc_val: 0.718310 time: 194.658102s\n",
      "Epoch: 0032 loss_train: 7.002031 acc_train: 0.750000 loss_val: 2.419559 acc_val: 0.713615 time: 200.802295s\n",
      "Epoch: 0033 loss_train: 7.141171 acc_train: 0.740566 loss_val: 2.366145 acc_val: 0.723005 time: 207.077179s\n",
      "Epoch: 0034 loss_train: 6.992740 acc_train: 0.754717 loss_val: 2.342284 acc_val: 0.699531 time: 213.670218s\n",
      "Epoch: 0035 loss_train: 7.316262 acc_train: 0.750000 loss_val: 2.363992 acc_val: 0.718310 time: 219.669026s\n",
      "Epoch: 0036 loss_train: 6.943326 acc_train: 0.750000 loss_val: 2.279682 acc_val: 0.718310 time: 225.459064s\n",
      "Epoch: 0037 loss_train: 6.909879 acc_train: 0.753538 loss_val: 2.298482 acc_val: 0.723005 time: 231.267236s\n",
      "Epoch: 0038 loss_train: 6.800919 acc_train: 0.767689 loss_val: 2.297536 acc_val: 0.718310 time: 237.042002s\n",
      "Epoch: 0039 loss_train: 6.767372 acc_train: 0.754717 loss_val: 2.400409 acc_val: 0.718310 time: 242.868446s\n",
      "Epoch: 0040 loss_train: 6.718923 acc_train: 0.770047 loss_val: 2.336877 acc_val: 0.713615 time: 248.991211s\n",
      "Epoch: 0041 loss_train: 6.636068 acc_train: 0.766509 loss_val: 2.321827 acc_val: 0.704225 time: 256.206952s\n",
      "Epoch: 0042 loss_train: 6.638930 acc_train: 0.766509 loss_val: 2.334492 acc_val: 0.680751 time: 262.171103s\n",
      "Epoch: 0043 loss_train: 6.505684 acc_train: 0.771226 loss_val: 2.435445 acc_val: 0.732394 time: 268.549400s\n",
      "Epoch: 0044 loss_train: 6.670301 acc_train: 0.768868 loss_val: 2.336658 acc_val: 0.708920 time: 276.003571s\n",
      "Epoch: 0045 loss_train: 6.551990 acc_train: 0.770047 loss_val: 2.328521 acc_val: 0.699531 time: 282.339993s\n",
      "Epoch: 0046 loss_train: 6.795564 acc_train: 0.774764 loss_val: 2.288643 acc_val: 0.708920 time: 288.497231s\n",
      "Epoch: 0047 loss_train: 6.574312 acc_train: 0.779481 loss_val: 2.338634 acc_val: 0.741784 time: 295.701942s\n",
      "Epoch: 0048 loss_train: 6.622637 acc_train: 0.770047 loss_val: 2.276511 acc_val: 0.708920 time: 302.889763s\n",
      "Epoch: 0049 loss_train: 6.638580 acc_train: 0.772406 loss_val: 2.236573 acc_val: 0.713615 time: 309.036931s\n",
      "Epoch: 0050 loss_train: 6.516501 acc_train: 0.779481 loss_val: 2.335454 acc_val: 0.737089 time: 315.120632s\n",
      "Epoch: 0051 loss_train: 6.462607 acc_train: 0.771226 loss_val: 2.267761 acc_val: 0.718310 time: 320.850647s\n",
      "Epoch: 0052 loss_train: 6.556152 acc_train: 0.786557 loss_val: 2.268597 acc_val: 0.694836 time: 327.142065s\n",
      "Epoch: 0053 loss_train: 6.423860 acc_train: 0.779481 loss_val: 2.341484 acc_val: 0.737089 time: 333.673066s\n",
      "Epoch: 0054 loss_train: 6.518446 acc_train: 0.792453 loss_val: 2.272556 acc_val: 0.708920 time: 340.344274s\n",
      "Epoch: 0055 loss_train: 6.367054 acc_train: 0.783019 loss_val: 2.357015 acc_val: 0.751174 time: 346.142174s\n",
      "Epoch: 0056 loss_train: 6.384987 acc_train: 0.787736 loss_val: 2.310311 acc_val: 0.732394 time: 351.909181s\n",
      "Epoch: 0057 loss_train: 6.388879 acc_train: 0.798349 loss_val: 2.297262 acc_val: 0.713615 time: 357.881591s\n",
      "Epoch: 0058 loss_train: 6.340546 acc_train: 0.794811 loss_val: 2.342135 acc_val: 0.746479 time: 363.885103s\n",
      "Epoch: 0059 loss_train: 6.256866 acc_train: 0.801887 loss_val: 2.297971 acc_val: 0.713615 time: 369.951084s\n",
      "Epoch: 0060 loss_train: 6.459401 acc_train: 0.790094 loss_val: 2.292070 acc_val: 0.708920 time: 376.552055s\n",
      "Epoch: 0061 loss_train: 6.331586 acc_train: 0.793632 loss_val: 2.355096 acc_val: 0.755869 time: 383.761619s\n",
      "Epoch: 0062 loss_train: 6.360436 acc_train: 0.779481 loss_val: 2.248817 acc_val: 0.741784 time: 390.092345s\n",
      "Epoch: 0063 loss_train: 6.336640 acc_train: 0.805425 loss_val: 2.243884 acc_val: 0.746479 time: 396.239895s\n",
      "Epoch: 0064 loss_train: 6.135531 acc_train: 0.793632 loss_val: 2.328766 acc_val: 0.746479 time: 402.548114s\n",
      "Epoch: 0065 loss_train: 6.283253 acc_train: 0.799528 loss_val: 2.254479 acc_val: 0.727700 time: 408.785646s\n",
      "Epoch: 0066 loss_train: 6.146065 acc_train: 0.799528 loss_val: 2.238467 acc_val: 0.741784 time: 414.924097s\n",
      "Epoch: 0067 loss_train: 6.068635 acc_train: 0.803066 loss_val: 2.226689 acc_val: 0.718310 time: 420.763870s\n",
      "Epoch: 0068 loss_train: 6.194432 acc_train: 0.807783 loss_val: 2.252053 acc_val: 0.732394 time: 426.719135s\n",
      "Epoch: 0069 loss_train: 6.090059 acc_train: 0.804245 loss_val: 2.317705 acc_val: 0.737089 time: 432.872836s\n",
      "Epoch: 0070 loss_train: 6.145959 acc_train: 0.812500 loss_val: 2.234224 acc_val: 0.727700 time: 438.658530s\n",
      "Epoch: 0071 loss_train: 5.983731 acc_train: 0.813679 loss_val: 2.274486 acc_val: 0.737089 time: 444.672240s\n",
      "Epoch: 0072 loss_train: 5.838681 acc_train: 0.816038 loss_val: 2.256907 acc_val: 0.727700 time: 450.703707s\n",
      "Epoch: 0073 loss_train: 6.142107 acc_train: 0.816038 loss_val: 2.295998 acc_val: 0.732394 time: 456.921367s\n",
      "Epoch: 0074 loss_train: 6.159225 acc_train: 0.808962 loss_val: 2.243384 acc_val: 0.727700 time: 462.939815s\n",
      "Epoch: 0075 loss_train: 5.968170 acc_train: 0.807783 loss_val: 2.307045 acc_val: 0.732394 time: 468.998006s\n",
      "Epoch: 0076 loss_train: 6.119652 acc_train: 0.804245 loss_val: 2.299070 acc_val: 0.718310 time: 474.976011s\n",
      "Epoch: 0077 loss_train: 5.896612 acc_train: 0.810142 loss_val: 2.286045 acc_val: 0.723005 time: 480.888280s\n",
      "Epoch: 0078 loss_train: 6.010503 acc_train: 0.798349 loss_val: 2.438814 acc_val: 0.727700 time: 487.157442s\n",
      "Epoch: 0079 loss_train: 6.005290 acc_train: 0.800708 loss_val: 2.291234 acc_val: 0.732394 time: 492.964968s\n",
      "Epoch: 0080 loss_train: 6.064807 acc_train: 0.804245 loss_val: 2.292908 acc_val: 0.741784 time: 498.782601s\n",
      "Epoch: 0081 loss_train: 5.932126 acc_train: 0.811321 loss_val: 2.341586 acc_val: 0.718310 time: 505.078186s\n",
      "Epoch: 0082 loss_train: 6.021944 acc_train: 0.812500 loss_val: 2.314046 acc_val: 0.746479 time: 511.487240s\n",
      "Epoch: 0083 loss_train: 5.997389 acc_train: 0.808962 loss_val: 2.225492 acc_val: 0.746479 time: 517.433316s\n",
      "Epoch: 0084 loss_train: 6.139798 acc_train: 0.804245 loss_val: 2.249462 acc_val: 0.723005 time: 523.465499s\n",
      "Epoch: 0085 loss_train: 6.046081 acc_train: 0.805425 loss_val: 2.325052 acc_val: 0.718310 time: 529.413404s\n",
      "Epoch: 0086 loss_train: 6.150469 acc_train: 0.801887 loss_val: 2.225832 acc_val: 0.746479 time: 535.446550s\n",
      "Epoch: 0087 loss_train: 5.929537 acc_train: 0.814858 loss_val: 2.213384 acc_val: 0.737089 time: 541.601898s\n",
      "Epoch: 0088 loss_train: 5.995923 acc_train: 0.811321 loss_val: 2.341189 acc_val: 0.723005 time: 547.773003s\n",
      "Epoch: 0089 loss_train: 5.872062 acc_train: 0.803066 loss_val: 2.174039 acc_val: 0.769953 time: 553.876109s\n",
      "Epoch: 0090 loss_train: 5.933188 acc_train: 0.813679 loss_val: 2.206879 acc_val: 0.755869 time: 560.183758s\n",
      "Epoch: 0091 loss_train: 5.963398 acc_train: 0.808962 loss_val: 2.157539 acc_val: 0.765258 time: 566.629430s\n",
      "Epoch: 0092 loss_train: 6.056340 acc_train: 0.806604 loss_val: 2.167867 acc_val: 0.774648 time: 572.551084s\n",
      "Epoch: 0093 loss_train: 5.921687 acc_train: 0.807783 loss_val: 2.267829 acc_val: 0.737089 time: 578.541843s\n",
      "Epoch: 0094 loss_train: 5.925143 acc_train: 0.817217 loss_val: 2.178236 acc_val: 0.784038 time: 584.777294s\n",
      "Epoch: 0095 loss_train: 6.032252 acc_train: 0.812500 loss_val: 2.352531 acc_val: 0.723005 time: 590.840956s\n",
      "Epoch: 0096 loss_train: 5.892039 acc_train: 0.806604 loss_val: 2.221413 acc_val: 0.769953 time: 596.837622s\n",
      "Epoch: 0097 loss_train: 5.751432 acc_train: 0.811321 loss_val: 2.297741 acc_val: 0.723005 time: 602.830268s\n",
      "Epoch: 0098 loss_train: 5.867101 acc_train: 0.813679 loss_val: 2.215783 acc_val: 0.784038 time: 608.867335s\n",
      "Epoch: 0099 loss_train: 5.796872 acc_train: 0.805425 loss_val: 2.252675 acc_val: 0.746479 time: 614.902325s\n",
      "Epoch: 0100 loss_train: 5.628262 acc_train: 0.816038 loss_val: 2.261199 acc_val: 0.746479 time: 620.922216s\n",
      "Optimization Finished! Total time elapsed: 620.924781\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold, (train_idx, val_idx) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(kfold\u001B[38;5;241m.\u001B[39msplit(np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(train_dataset)))):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m     val_acc, val_loss \u001B[38;5;241m=\u001B[39m train_fold(train_idx, val_idx)\n\u001B[1;32m      7\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend((val_acc, val_loss))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Now train on the full training dataset (optional if you want a final model)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Test the model\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Output cross-validation and test results\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args[\"batch_size\"], shuffle=False)\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(train_dataset)))):\n",
    "    print(f\"Training fold {fold + 1}\")\n",
    "    val_acc, val_loss = train_fold(train_idx, val_idx)\n",
    "    results.append((val_acc, val_loss))\n",
    "\n",
    "# Now train on the full training dataset (optional if you want a final model)\n",
    "# Test the model\n",
    "\n",
    "# Output cross-validation and test results\n",
    "print(f\"Cross-validation results: {results}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T09:36:38.497767Z",
     "start_time": "2024-04-15T09:26:17.015979Z"
    }
   },
   "id": "8c24a2e8d47050d5",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
